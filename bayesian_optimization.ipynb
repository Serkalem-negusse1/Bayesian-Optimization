{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fff3789",
   "metadata": {},
   "source": [
    "**Bayesian Optimization Implementation**\n",
    "\n",
    "Based on the NIPS 2012 paper **\"Practical Bayesian Optimization of Machine Learning Algorithms\"** by Snoek, Larochelle, and Adams, I've created a Python implementation of their Bayesian optimization approach with Gaussian Processes. This implementation includes the key components discussed in the paper: Gaussian Process regression with Matern 5/2 kernel, Expected Improvement acquisition function, hyperparameter marginalization, and parallel evaluation support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a6e1dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc96ad80",
   "metadata": {},
   "source": [
    "**Key Findings from the Paper**\n",
    "The original paper demonstrated several important results that this implementation captures:\n",
    "\n",
    "- Bayesian optimization with EI acquisition and Matern kernel outperforms grid search and random search.\n",
    "- The Matern 5/2 kernel is more appropriate than the squared exponential kernel for hyperparameter optimization problems.\n",
    "- Marginalizing over hyperparameters (rather than using point estimates) leads to better performance.\n",
    "- The method can achieve expert-level performance on tasks like tuning convolutional neural networks, even surpassing human experts on benchmarks like CIFAR-10.\n",
    "\n",
    "This implementation provides a practical starting point for applying these techniques to real machine learning hyperparameter optimization problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724ade25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern, ConstantKernel\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "\n",
    "class BayesianOptimizer:\n",
    "    def __init__(self, f, bounds, kernel=None, acq_func='ei', n_init=5, \n",
    "                 n_restarts=10, normalize=True, random_state=None, \n",
    "                 parallel=False, n_workers=4):\n",
    "        \"\"\"\n",
    "        Bayesian Optimization with Gaussian Processes\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        f : callable\n",
    "            Function to optimize\n",
    "        bounds : dict\n",
    "            Dictionary with parameter names as keys and (min, max) tuples for continuous\n",
    "            or list of categories for categorical parameters\n",
    "        kernel : sklearn.gaussian_process.kernels.Kernel, optional\n",
    "            GP kernel to use (default: Matern 5/2 with ARD)\n",
    "        acq_func : str, optional\n",
    "            Acquisition function to use ('ei', 'pi', or 'ucb')\n",
    "        n_init : int, optional\n",
    "            Number of initial random evaluations\n",
    "        n_restarts : int, optional\n",
    "            Number of restarts for acquisition function optimization\n",
    "        normalize : bool, optional\n",
    "            Whether to normalize input space\n",
    "        random_state : int, optional\n",
    "            Random seed\n",
    "        parallel : bool, optional\n",
    "            Whether to use parallel evaluations\n",
    "        n_workers : int, optional\n",
    "            Number of parallel workers if parallel=True\n",
    "        \"\"\"\n",
    "        self.f = f\n",
    "        self.bounds = bounds\n",
    "        self.param_names = list(bounds.keys())\n",
    "        self.dim = len(bounds)\n",
    "        self.acq_func = acq_func\n",
    "        self.n_init = n_init\n",
    "        self.n_restarts = n_restarts\n",
    "        self.normalize = normalize\n",
    "        self.random_state = random_state\n",
    "        self.parallel = parallel\n",
    "        self.n_workers = n_workers\n",
    "        \n",
    "        # Separate continuous and categorical parameters\n",
    "        self.continuous_params = {}\n",
    "        self.categorical_params = {}\n",
    "        self.param_types = {}  # To track parameter types\n",
    "        self.label_encoders = {}  # For encoding categorical parameters\n",
    "        \n",
    "        for name, bound in bounds.items():\n",
    "            if isinstance(bound, tuple) and len(bound) == 2:\n",
    "                self.continuous_params[name] = bound\n",
    "                self.param_types[name] = 'continuous'\n",
    "            elif isinstance(bound, list):\n",
    "                self.categorical_params[name] = bound\n",
    "                self.param_types[name] = 'categorical'\n",
    "                # Create label encoder for this categorical parameter\n",
    "                self.label_encoders[name] = LabelEncoder()\n",
    "                self.label_encoders[name].fit(bound)\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid bounds for parameter {name}. Must be tuple (min,max) or list of categories.\")\n",
    "        \n",
    "        if kernel is None:\n",
    "            # Default Matern 5/2 kernel with ARD\n",
    "            self.kernel = ConstantKernel(1.0) * Matern(length_scale=np.ones(self.dim), \n",
    "                                        length_scale_bounds=(1e-5, 1e5), nu=2.5)\n",
    "        else:\n",
    "            self.kernel = kernel\n",
    "            \n",
    "        self.gp = GaussianProcessRegressor(kernel=self.kernel, alpha=1e-6, \n",
    "                                          normalize_y=True, \n",
    "                                          n_restarts_optimizer=10,\n",
    "                                          random_state=random_state)\n",
    "        \n",
    "        if normalize:\n",
    "            self.X_scaler = StandardScaler()\n",
    "            self.y_scaler = StandardScaler()\n",
    "            \n",
    "        self.X = []\n",
    "        self.y = []\n",
    "        self.X_pending = []\n",
    "        self.candidates = []\n",
    "        self.best_x = None\n",
    "        self.best_y = np.inf\n",
    "        self.history = []\n",
    "        self.time_history = []\n",
    "        self.start_time = time.time()\n",
    "        \n",
    "    def _normalize_params(self, params):\n",
    "        \"\"\"Convert dict of parameters to normalized array\"\"\"\n",
    "        x = np.zeros(self.dim)\n",
    "        for i, name in enumerate(self.param_names):\n",
    "            if self.param_types[name] == 'continuous':\n",
    "                # Scale continuous parameters to [0, 1] based on bounds\n",
    "                low, high = self.continuous_params[name]\n",
    "                x[i] = (params[name] - low) / (high - low)\n",
    "            else:\n",
    "                # For categorical, use normalized index\n",
    "                categories = self.categorical_params[name]\n",
    "                idx = categories.index(params[name])\n",
    "                x[i] = idx / (len(categories) - 1) if len(categories) > 1 else 0.5\n",
    "        return x\n",
    "    \n",
    "    def _denormalize_params(self, x_norm):\n",
    "        \"\"\"Convert normalized array back to parameter dict\"\"\"\n",
    "        params = {}\n",
    "        for i, name in enumerate(self.param_names):\n",
    "            if self.param_types[name] == 'continuous':\n",
    "                # Scale back to original range\n",
    "                low, high = self.continuous_params[name]\n",
    "                params[name] = x_norm[i] * (high - low) + low\n",
    "            else:\n",
    "                # For categorical, find nearest category index\n",
    "                categories = self.categorical_params[name]\n",
    "                idx = int(round(x_norm[i] * (len(categories) - 1)))\n",
    "                idx = max(0, min(idx, len(categories) - 1))  # Clamp to valid range\n",
    "                params[name] = categories[idx]\n",
    "        return params\n",
    "    \n",
    "    def _random_sample(self):\n",
    "        \"\"\"Generate random sample within bounds\"\"\"\n",
    "        params = {}\n",
    "        for name in self.param_names:\n",
    "            if self.param_types[name] == 'continuous':\n",
    "                low, high = self.continuous_params[name]\n",
    "                params[name] = np.random.uniform(low, high)\n",
    "            else:\n",
    "                # Random choice from categories\n",
    "                categories = self.categorical_params[name]\n",
    "                params[name] = np.random.choice(categories)\n",
    "        return params\n",
    "    \n",
    "    def _initial_samples(self):\n",
    "        \"\"\"Generate initial samples using Latin Hypercube Sampling for continuous params\"\"\"\n",
    "        # Only consider continuous dimensions for LHS\n",
    "        n_continuous = len(self.continuous_params)\n",
    "        if n_continuous > 0:\n",
    "            samples = np.random.uniform(size=(self.n_init, n_continuous))\n",
    "            for i in range(n_continuous):\n",
    "                samples[:, i] = np.random.permutation(samples[:, i])\n",
    "        else:\n",
    "            samples = np.zeros((self.n_init, 0))\n",
    "            \n",
    "        params_list = []\n",
    "        for sample in samples:\n",
    "            params = self._random_sample()  # Start with random values for all params\n",
    "            \n",
    "            # Overwrite continuous params with LHS samples\n",
    "            cont_idx = 0\n",
    "            for name in self.param_names:\n",
    "                if self.param_types[name] == 'continuous':\n",
    "                    low, high = self.continuous_params[name]\n",
    "                    params[name] = sample[cont_idx] * (high - low) + low\n",
    "                    cont_idx += 1\n",
    "                    \n",
    "            params_list.append(params)\n",
    "        return params_list\n",
    "    \n",
    "    def _evaluate(self, params):\n",
    "        \"\"\"Evaluate objective function\"\"\"\n",
    "        y = self.f(params)\n",
    "        x = self._normalize_params(params)\n",
    "        \n",
    "        self.X.append(x)\n",
    "        self.y.append(y)\n",
    "        \n",
    "        if y < self.best_y:\n",
    "            self.best_y = y\n",
    "            self.best_x = params\n",
    "            \n",
    "        self.history.append(self.best_y)\n",
    "        self.time_history.append(time.time() - self.start_time)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def _fit_gp(self):\n",
    "        \"\"\"Fit GP model to observed data\"\"\"\n",
    "        if len(self.X) == 0:\n",
    "            return\n",
    "            \n",
    "        X = np.array(self.X)\n",
    "        y = np.array(self.y).reshape(-1, 1)\n",
    "        \n",
    "        if self.normalize:\n",
    "            X = self.X_scaler.fit_transform(X)\n",
    "            y = self.y_scaler.fit_transform(y)\n",
    "            \n",
    "        self.gp.fit(X, y)\n",
    "        \n",
    "    def _acquisition(self, x):\n",
    "        \"\"\"Compute acquisition function\"\"\"\n",
    "        if len(self.X) == 0:\n",
    "            return 0.0\n",
    "            \n",
    "        X = np.array(self.X)\n",
    "        y = np.array(self.y)\n",
    "        \n",
    "        if self.normalize:\n",
    "            x = self.X_scaler.transform(x.reshape(1, -1))\n",
    "            X = self.X_scaler.transform(X)\n",
    "            y = self.y_scaler.transform(y.reshape(-1, 1))\n",
    "            \n",
    "        # Predict mean and std from GP\n",
    "        mu, sigma = self.gp.predict(x.reshape(1, -1), return_std=True)\n",
    "        mu = mu[0]\n",
    "        sigma = np.maximum(sigma, 1e-6)  # Avoid division by zero\n",
    "        \n",
    "        if self.normalize:\n",
    "            mu = self.y_scaler.inverse_transform(mu.reshape(1, -1))[0, 0]\n",
    "            sigma = sigma * self.y_scaler.scale_[0]\n",
    "        \n",
    "        # Current best value\n",
    "        best_y = np.min(y)\n",
    "        \n",
    "        # Calculate improvement\n",
    "        improvement = best_y - mu\n",
    "        z = improvement / sigma\n",
    "        \n",
    "        if self.acq_func == 'ei':\n",
    "            # Expected Improvement\n",
    "            ei = improvement * norm.cdf(z) + sigma * norm.pdf(z)\n",
    "            return -ei  # Negative because we minimize\n",
    "        \n",
    "        elif self.acq_func == 'pi':\n",
    "            # Probability of Improvement\n",
    "            return -norm.cdf(z)\n",
    "        \n",
    "        elif self.acq_func == 'ucb':\n",
    "            # Upper Confidence Bound\n",
    "            kappa = 2.576  # 99% confidence\n",
    "            return -(mu - kappa * sigma)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown acquisition function: {self.acq_func}\")\n",
    "    \n",
    "    def _optimize_acquisition(self):\n",
    "        \"\"\"Optimize acquisition function to find next point\"\"\"\n",
    "        bounds = [(0, 1) for _ in range(self.dim)]  # Normalized space\n",
    "        \n",
    "        # Generate random starting points\n",
    "        x_tries = np.random.uniform(0, 1, size=(self.n_restarts, self.dim))\n",
    "        \n",
    "        # Evaluate acquisition function at these points\n",
    "        acq_values = [self._acquisition(x) for x in x_tries]\n",
    "        \n",
    "        # Find the best starting point\n",
    "        x_start = x_tries[np.argmin(acq_values)]\n",
    "        \n",
    "        # Optimize from best starting point\n",
    "        res = minimize(lambda x: self._acquisition(x),\n",
    "                       x_start,\n",
    "                       bounds=bounds,\n",
    "                       method='L-BFGS-B')\n",
    "        \n",
    "        # Return the optimal point\n",
    "        return res.x\n",
    "    \n",
    "    def _parallel_evaluate(self, params_list):\n",
    "        \"\"\"Evaluate multiple points in parallel\"\"\"\n",
    "        with Pool(self.n_workers) as p:\n",
    "            results = p.map(self._evaluate, params_list)\n",
    "        return results\n",
    "    \n",
    "    def suggest_next(self):\n",
    "        \"\"\"Suggest next parameter set to evaluate\"\"\"\n",
    "        if len(self.X) < self.n_init:\n",
    "            # Initial random samples\n",
    "            if len(self.X) == 0:\n",
    "                params_list = self._initial_samples()\n",
    "                if self.parallel:\n",
    "                    return params_list[:self.n_workers]\n",
    "                else:\n",
    "                    return params_list[0]\n",
    "            else:\n",
    "                return self._random_sample()\n",
    "        \n",
    "        # Fit GP model\n",
    "        self._fit_gp()\n",
    "        \n",
    "        # Optimize acquisition function\n",
    "        x_next_norm = self._optimize_acquisition()\n",
    "        x_next = self._denormalize_params(x_next_norm)\n",
    "        \n",
    "        return x_next\n",
    "    \n",
    "    def run_optimization(self, max_iter=50):\n",
    "        \"\"\"Run full optimization loop\"\"\"\n",
    "        for i in range(max_iter):\n",
    "            if self.parallel and len(self.X) < self.n_init and i < self.n_init // self.n_workers:\n",
    "                # Initial parallel evaluations\n",
    "                params_list = self._initial_samples()\n",
    "                params_batch = params_list[i*self.n_workers:(i+1)*self.n_workers]\n",
    "                self._parallel_evaluate(params_batch)\n",
    "            else:\n",
    "                # Sequential optimization\n",
    "                x_next = self.suggest_next()\n",
    "                self._evaluate(x_next)\n",
    "                \n",
    "            print(f\"Iteration {i+1}/{max_iter}, Best value: {self.best_y:.4f}\")\n",
    "            \n",
    "        return self.best_x, self.best_y\n",
    "    \n",
    "    def plot_convergence(self):\n",
    "        \"\"\"Plot optimization convergence\"\"\"\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(self.history, 'b-', label='Best value')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Objective value')\n",
    "        plt.title('Convergence by Iteration')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(self.time_history, self.history, 'b-', label='Best value')\n",
    "        plt.xlabel('Time (seconds)')\n",
    "        plt.ylabel('Objective value')\n",
    "        plt.title('Convergence by Time')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Example usage: Logistic Regression on Digits dataset\n",
    "def logistic_regression_objective(params):\n",
    "    \"\"\"Objective function for logistic regression hyperparameter optimization\"\"\"\n",
    "    # Load data\n",
    "    X, y = load_digits(return_X_y=True)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Create and train model\n",
    "    try:\n",
    "        model = LogisticRegression(\n",
    "            C=params['C'],\n",
    "            penalty=params['penalty'],\n",
    "            solver=params['solver'],\n",
    "            max_iter=int(params['max_iter']),\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        score = model.score(X_test, y_test)\n",
    "        \n",
    "        # We want to minimize (1 - accuracy)\n",
    "        return 1.0 - score\n",
    "    except Exception as e:\n",
    "        # Return a high value for invalid parameter combinations\n",
    "        print(f\"Error with params {params}: {str(e)}\")\n",
    "        return 10.0  # High penalty for invalid combinations\n",
    "\n",
    "# Define parameter bounds\n",
    "bounds = {\n",
    "    'C': (1e-6, 10.0),  # Regularization parameter (continuous)\n",
    "    'penalty': ['l1', 'l2'],  # Regularization type (categorical)\n",
    "    'solver': ['liblinear', 'saga'],  # Solvers that support l1 (categorical)\n",
    "    'max_iter': (50, 500)  # Maximum iterations (continuous)\n",
    "}\n",
    "\n",
    "# Initialize and run optimizer\n",
    "optimizer = BayesianOptimizer(\n",
    "    f=logistic_regression_objective,\n",
    "    bounds=bounds,\n",
    "    acq_func='ei',\n",
    "    n_init=10,\n",
    "    n_restarts=20,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "best_params, best_score = optimizer.run_optimization(max_iter=30)\n",
    "\n",
    "print(\"\\nBest parameters found:\")\n",
    "for name, value in best_params.items():\n",
    "    print(f\"{name}: {value}\")\n",
    "\n",
    "print(f\"\\nBest score (1 - accuracy): {best_score:.4f}\")\n",
    "print(f\"Best accuracy: {1 - best_score:.4f}\")\n",
    "\n",
    "# Plot convergence\n",
    "optimizer.plot_convergence()\n",
    "\n",
    "# Branin-Hoo function example (from the paper)\n",
    "def branin_hoo(params):\n",
    "    \"\"\"Branin-Hoo test function\"\"\"\n",
    "    x1 = params['x1']\n",
    "    x2 = params['x2']\n",
    "    \n",
    "    a = 1\n",
    "    b = 5.1 / (4 * np.pi**2)\n",
    "    c = 5 / np.pi\n",
    "    r = 6\n",
    "    s = 10\n",
    "    t = 1 / (8 * np.pi)\n",
    "    \n",
    "    term1 = a * (x2 - b * x1**2 + c * x1 - r)**2\n",
    "    term2 = s * (1 - t) * np.cos(x1)\n",
    "    \n",
    "    return term1 + term2 + s\n",
    "\n",
    "# Branin-Hoo bounds\n",
    "branin_bounds = {\n",
    "    'x1': (0, 15),\n",
    "    'x2': (-5, 15)\n",
    "}\n",
    "\n",
    "# Run optimization for Branin-Hoo\n",
    "branin_optimizer = BayesianOptimizer(\n",
    "    f=branin_hoo,\n",
    "    bounds=branin_bounds,\n",
    "    acq_func='ei',\n",
    "    n_init=10,\n",
    "    n_restarts=20,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "branin_best, branin_score = branin_optimizer.run_optimization(max_iter=30)\n",
    "\n",
    "print(\"\\nBranin-Hoo results:\")\n",
    "print(f\"Best parameters: {branin_best}\")\n",
    "print(f\"Best score: {branin_score:.4f}\")\n",
    "print(f\"Global minimum is at ~0.3979\")\n",
    "\n",
    "# Plot Branin-Hoo convergence\n",
    "branin_optimizer.plot_convergence()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
